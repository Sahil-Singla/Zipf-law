# -*- coding: utf-8 -*-
"""2016A7PS0103P

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v4VdDPuMGJXdSgSgYqhZXmJXQTcD6BaE
"""

import pandas as pd
from matplotlib import pyplot as plt
import re
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import word_tokenize
from nltk import word_tokenize
from nltk.util import ngrams
from collections import Counter
from nltk.stem import PorterStemmer    
from nltk.stem import WordNetLemmatizer

#open the file to read, worked on AK/wiki_01
with open('wiki_01') as text_file:
  html_text = BeautifulSoup(text_file,'html.parser')
print(html_text)

tags = ['a','doc']
#simple function to get text inside <a></a> tags. Example: <a href="www.google.com"> Google </a> gives Google.
def getString(s):
    a_str = s.string
    if(a_str):
        return a_str
    else:
        return ""
for t in tags:
    #print(html(t))
    sentences = html_text(t)
    if(t=='a'):
        for s in sentences:
         extracted = getString(s)
         s.replace_with(extracted)  
full_text = ""
for plain in html_text.find_all():
    full_text = full_text+plain.text   
#full_text = full_text.lower()      
filtered = re.sub('[^A-Za-z0-9]+', ' ', full_text)

print(full_text)

#using regex expression to extract relevant data
filtered = re.sub('[^A-Za-z0-9]+', ' ', full_text)
words = nltk.word_tokenize(filtered)
print(words)

len(words)

'''
#function that forms ngrams
@params: 1) n in ngrams
         2) words is the list of words to form ngrams from
'''
def form_ngrams(n, words):
    ngrams = dict()
    for i in range(len(words)- n + 1):
        key = []
        for j in range(i,i+n):
          key.append(words[j])
          key_t = tuple(key)
        if key_t in ngrams:
            ngrams[key_t] += 1
        else:
            ngrams[key_t] = 1
    ngrams = sorted(ngrams.items(), key=lambda x: -x[1])
    return ngrams

'''
#function that finds number of ngrams needed to cover a particular percentage of corpus
@params: 1) percentage of the corpus to be covered
         2) dict of ngrams
'''
def distribution_by_percentage(percentage, ngram):
    freq_sum = 0
    for row in ngram:
      freq_sum+=row[1]
    required = 0
    present = 0
    for row in ngram:
        present += 1
        required += row[1]
        if required/freq_sum > percentage/100:
            break
    return present

unigrams = form_ngrams(1, words)
bigrams = form_ngrams(2, words)
trigrams = form_ngrams(3, words)
print(len(unigrams))
print(len(bigrams))
print(len(trigrams))

print(distribution_by_percentage(90,unigrams))
print(distribution_by_percentage(80,bigrams))
print(distribution_by_percentage(70,trigrams))

print(unigrams)

'''
#function to plot graph on log scale
@params: 1) ngrams
'''
def plot_graph(ngrams):
    y = [freq[1] for freq in ngrams]
    x = [rank for rank in range(1,len(ngrams)+1)]
    plt.xlabel ('WordRank')
    plt.ylabel ('Frequency')
    plt.yscale('log')
    plt.xscale('log')
    plt.plot(x, y)

plot_graph(unigrams)

plot_graph(bigrams)

plot_graph(trigrams)

from nltk.stem import PorterStemmer    
ps = PorterStemmer()    
stemmed = []
for w in words: 
    stemmed.append(ps.stem(w))

stemmed_unigrams = form_ngrams(1, stemmed)
stemmed_bigrams = form_ngrams(2, stemmed)
stemmed_trigrams = form_ngrams(3, stemmed)
print(len(stemmed_unigrams))
print(len(stemmed_bigrams))
print(len(stemmed_trigrams))

print(distribution_by_percentage(90,stemmed_unigrams))
print(distribution_by_percentage(80,stemmed_bigrams))
print(distribution_by_percentage(70,stemmed_trigrams))

plot_graph(stemmed_unigrams)

plot_graph(stemmed_bigrams)

plot_graph(stemmed_trigrams)

from nltk.stem import WordNetLemmatizer 
lmtzr = WordNetLemmatizer()   
lemmatized = []
for w in words: 
    lemmatized.append(lmtzr.lemmatize(w))

lemmatized_unigrams = form_ngrams(1, lemmatized)
lemmatized_bigrams = form_ngrams(2, lemmatized)
lemmatized_trigrams = form_ngrams(3, lemmatized)
print(len(lemmatized_unigrams))
print(len(lemmatized_bigrams))
print(len(lemmatized_trigrams))

print(distribution_by_percentage(90,lemmatized_unigrams))
print(distribution_by_percentage(80,lemmatized_bigrams))
print(distribution_by_percentage(70,lemmatized_trigrams))

plot_graph(lemmatized_unigrams)

plot_graph(lemmatized_bigrams)

plot_graph(lemmatized_trigrams)

'''
#function that performs chi_square test
@params: 1) v1: count of word1 in the bigram
         2) v2: count of word2 in the bigram
         3) v3: count of the bigram itself
         4) total: sum of bigrams frequencies
'''
def chi_square(v1,v2,v3,total):
  c11 = v3
  c12 = v1-v3
  c21 = v2-v3
  c22 = total-v1-v2
  #print(c11,c12,c21,c22)
  dividend = (c11+c12)*(c11+c21)*(c12+c22)*(c21+c22)
  r = total*((c11*c22-c12*c21)^2)/dividend
  #print(r)
  return r

def create_dict_from_list(unigrams):
  unigrams_dict = dict()
  for uni in unigrams:
    #print(uni[0][0])
    unigrams_dict[uni[0][0]] = uni[1]
  #print(unigrams_dict)  
  return unigrams_dict

chi_sq_results = []
total=0
for bg in bigrams:
  total+=bg[1]
unigrams_dict = create_dict_from_list(unigrams)  
#print(total)  
for bg in bigrams:
  #print(bg[0][0])
  v1 = unigrams_dict.get(bg[0][0])
  v2 = unigrams_dict.get(bg[0][1])
  v3 = bg[1]
  #print(v1,v2,v3)
  #print(v3)
  #print(bg)
  chi_sq_results.append((bg[0],chi_square(v1,v2,v3,total)))
sorted_results = sorted(chi_sq_results, key=lambda x: -x[1])

print(sorted_results)

top20 = sorted_results[:20]

print(top20)

